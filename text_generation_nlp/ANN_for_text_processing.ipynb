{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section is about text generation and processing using an ANN \n",
    "\n",
    "The basic building block of an ANN is the perceptron. It is based on a simplication of the biological neuron. Multiple inputs are provided to the perceptron. They are each provided a weight and added to a bias. The body of the perceptron contains the activation function, which linearly combines the inputs but then gives a non-linear output (typically a value of 0 or 1) and feeds that into the next layer.  \n",
    "\n",
    "The weights $w_i$ are initialized randomly\n",
    "\n",
    "$$ \\text{output} = \\sum w_i x_i + b $$    \n",
    "\n",
    "\n",
    "A densely connected neural network is just a series of perceptrons that are connected to one another via their inputs and outputs. A network with three or more input layers is known as a deep network. \n",
    "\n",
    "Changing the activation function can be useful depending on the task. Common ones include: \n",
    "\n",
    "- The sigmoid (0 to 1)  \n",
    "- Tanh (-1 to 1)  \n",
    "- ReLu (0 to z), just max(0,z)  \n",
    "- Boxcar (0 or 1)  \n",
    "\n",
    "ReLu seems to have the best performance in most situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use Keras on the classic IRIS daraset to produce a simple ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For keras, we need to make each of the class labels into one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class 0 --> [1,0,0]\n",
    "#class 1 -- > [0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to make a simple ANN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8,input_dim=4,activation='relu'))\n",
    "model.add(Dense(8,input_dim=4,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(scaled_X_train,y_train,epochs=150,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predictions,y_test.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.argmax(axis=1),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#easy way of saving a model to disk\n",
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN theory overview   \n",
    "\n",
    "RNNs are specifially designed to work with sequence data because they can conserve information about datapoints that occured earlier in the sequence. A sequence is a vector whose order matters -> basically we want to be able to predict the sequence shifted over some number of timesteps. \n",
    "\n",
    "In a normal Feed Forward netowork the output only goes to the next cells in the ANN. In a recurrent neuron, the output from previous timesteps will feed into the cell determining the values at the current timestep. Its easy to create a layer of recurrent neurons, where the outputs go back into each of the perceptrons in the network.  \n",
    "\n",
    "The output at timestep t is a function of all the previous outputs, so the network has some memory of previous information. These previous datapoints can easily be weighted. \n",
    "\n",
    "We can perform a sequence to sequence output - shift a given sequence into the future. \n",
    "\n",
    "We can also pass in a sequence of and output a vector (sentiment scores, for example). This would be associated with a training dataset where this mapping is in place. \n",
    "\n",
    "Vector to sequence is also possible. For example, we have a seed word 'hello' and the network will predict the next part of the phrase. \n",
    "\n",
    "### LSTM cells \n",
    "\n",
    "Simple RNNs can begin to forget data that it was trained on a while ago. The LSTM cell was developed in order to counter this.   \n",
    "\n",
    "The input to an LSTM are h(t-1) and x(t) in addtion to the previous cell state c(t-1)\n",
    "\n",
    "The first step is called the forget gate - what will be lost from the cell state?  Pass h(t-1) and x(t) into a sigmoid. 1 means keep, 0 means forget -> this is f(t)\n",
    "\n",
    "The next step is to decide what new information is to be stored in the new state, c(t). Take in h(t-1) and x(t), pass into sigmoid (i(t)) and also into a tanh() layer. This makes candidate values that can be added to the state. We then combine what comes from the sigmoid with what comes from the tanh(). \n",
    "\n",
    "Multiply the old state by f(t) then add scaled candiate values i(t). \n",
    "\n",
    "There is then a final combination stage. \n",
    "\n",
    "The Gated Recurrent Unit (GRU) is much the same idea, only has a simpler architecture.   \n",
    "\n",
    "LSTMs seem to work best with text data - we're going to use them here for text generation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en',disable=['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    \n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text  not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens) #all the tokens (not punctuation) in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea is this: We pass in a sentence and than have the network predict the next word. We have 25 words and we will then predict the 26th word. \n",
    "Then the window will slide along by one word and the prediction will be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1 \n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len,len(tokens)):\n",
    "    \n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(text_sequences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are just all the sequencnes that we're trying to predict in the training dataset. Given the preceeding words, can the network predict the last word of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each of thes numbers in these sequencies is an id that links to a particular word. A dictionary object is created, which maps between them. \n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This allows you to do count vectorization - counts the number of words\n",
    "#tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the token ids to an array object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1] #features\n",
    "y = sequences[:,-1] #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set one-hot encoding\n",
    "y = to_categorical(y,num_classes=vocab_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size,seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,seq_len,input_length=seq_len))\n",
    "    model.add(LSTM(seq_len*2,return_sequences=True))\n",
    "    model.add(LSTM(seq_len*2))\n",
    "    model.add(Dense(seq_len*2,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(vocab_size,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 25, 25)            67950     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 25, 50)            15200     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2718)              138618    \n",
      "=================================================================\n",
      "Total params: 244,518\n",
      "Trainable params: 244,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size+1,X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11312/11312 [==============================] - 9s 795us/step - loss: 7.0494 - acc: 0.0357\n",
      "Epoch 2/2\n",
      "11312/11312 [==============================] - 7s 652us/step - loss: 6.3755 - acc: 0.0529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb427d13c8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We don't really have a test-train split, we're just looking at the training dataset. We are going to use the output to generate some new text\n",
    "\n",
    "model.fit(X,y,batch_size=128,epochs=2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_mobydick_model.h5')\n",
    "dump(tokenizer,open('my_simpletokenizer','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,seq_len,seed_text,num_words):\n",
    "    \n",
    "    output_text  = []\n",
    "    \n",
    "    input_text = seed_text #need to feed it some seed -> will take in and generate one word. Then we chop the first word and feed to model again\n",
    "    \n",
    "    for i in range(num_words):\n",
    "        \n",
    "        \n",
    "        #Transform input\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        #of the text is too short or too long, will need to pad -> chops the first word \n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_len,truncating='pre')\n",
    "        \n",
    "        #predict the probs for each word and get the index of the word with the highest prob\n",
    "        predicted_word_ind = model.predict_classes(pad_encoded,verbose=0)[0] \n",
    "        \n",
    "        #get the word assciated with the predicted index\n",
    "        pred_word = tokenizer.index_word[predicted_word_ind]\n",
    "        \n",
    "        #add the predicted word to the input text\n",
    "        input_text += ' '+pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "        #after this, its going to chop the first word from the sequnece and then proceed forwards to predict word \n",
    "        #by word\n",
    "        \n",
    "    \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9521"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\n"
     ]
    }
   ],
   "source": [
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the the the the the the the the the the the the the the the the the the the the the the the the the'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len=X.shape[1],seed_text=seed_text,num_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('epochBIG.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load(open('epochBIG','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to be seen there was no bad olfactories my own letter was cheerily listening over his hearers who 's more can go have a wearing\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len=X.shape[1],seed_text=seed_text,num_words=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot learning \n",
    "\n",
    "We will be following a paper called 'end to end memory networks' \n",
    "\n",
    "The model takes in a discrete sets of inputs X1 - Xn, which are to be stored in memory, a query q and it then outputs an answer a\n",
    "\n",
    "Each of the x, q and a contain symbols coming from a dictionary with V (vocabulary) words \n",
    "\n",
    "The model writes all x to the memory up to a fixed buffer size, then finds a continuous representation for the x and q. \n",
    "\n",
    "We have input memory representation - what stories will go in - then output memory representation and generation of the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
