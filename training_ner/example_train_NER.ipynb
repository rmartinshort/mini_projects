{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3173dabc-7ee5-43fc-b50c-6d5210da1cc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using Huggingface Datasets and Transformers to train a NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5a952-efe8-410e-b5fa-800e5edee12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0015d-ecbc-4a22-a587-79911bdb7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30484c1e-caff-45e1-bf37-545babe816aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import list_datasets, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3832ff8-51ec-43e7-a486-2d09a0b4982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "003049cf-c7c2-476a-a2ef-b40936ffdea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef7e42-6c8a-41ba-b2fb-133672ed4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366b8d4-f2ca-4901-990b-d7303743e2c8",
   "metadata": {},
   "source": [
    "Prepare the dataset to be in the expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1c6ffd2c-4709-4215-8b6a-1ceeeeedb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_training_set(input_ner_pd,test_fraction=0.1,valid_fraction=0.1):\n",
    "    \n",
    "    input_ner_pd[\"labels\"] = input_ner_pd[\"labels\"].apply(lambda x: \"O\" if x == \"0\" else x)\n",
    "    \n",
    "    class_names = ner_data[\"labels\"].unique().tolist()\n",
    "    class_names_map = {class_names[i]:i for i in range(len(class_names))}\n",
    "    \n",
    "    input_ner_pd[\"int_label\"] = input_ner_pd[\"labels\"].apply(lambda x: int(class_names_map[str(x)]))\n",
    "    \n",
    "    sentences = input_ner_pd.\\\n",
    "        groupby(\"sentence_id\").\\\n",
    "        apply(lambda x: [list(x['words']), list(x['int_label'])]).\\\n",
    "        apply(pd.Series).\\\n",
    "        reset_index()\n",
    "    \n",
    "    sentences.columns=[\"id\",\"tokens\",\"ner_tags\"]\n",
    "    \n",
    "    X = sentences[[\"id\",\"tokens\"]]\n",
    "    y = sentences[\"ner_tags\"]\n",
    "\n",
    "    if test_fraction > 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_fraction, random_state=1)\n",
    "    else:\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        X_test = None\n",
    "        y_test = None\n",
    "    \n",
    "    if valid_fraction > 0:\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_fraction, random_state=1)\n",
    "    else:\n",
    "        X_valid = None\n",
    "        y_test = None\n",
    "        \n",
    "    nsets = 1\n",
    "    training_set = X_train.join(y_train).reset_index(drop=True)\n",
    "    if isinstance(X_test,pd.DataFrame):\n",
    "        test_set = X_test.join(y_test).reset_index(drop=True)\n",
    "        nsets = 2\n",
    "    if isinstance(X_valid,pd.DataFrame):\n",
    "        valid_set = X_valid.join(y_valid).reset_index(drop=True)\n",
    "        nsets = 3\n",
    "        \n",
    "    # Get NER features schema\n",
    "    ner_features = datasets.Features(\n",
    "            {\"id\": datasets.Value(\"int64\"),\n",
    "             \"tokens\": datasets.Sequence(datasets.Value(\"string\")),\n",
    "             \"ner_tags\": datasets.Sequence(datasets.ClassLabel(names=class_names))}\n",
    "            )\n",
    "    \n",
    "    # Make Transformers dataset object\n",
    "    if nsets == 1:\n",
    "        ner_dataset = DatasetDict({\n",
    "            \"train\":Dataset.from_pandas(training_set,features=ner_features)\n",
    "        })\n",
    "    elif nsets == 2:\n",
    "        ner_dataset = DatasetDict({\n",
    "            \"train\":Dataset.from_pandas(training_set,features=ner_features),\n",
    "            \"test\":Dataset.from_pandas(test_set,features=ner_features)\n",
    "        })\n",
    "    else:\n",
    "        ner_dataset = DatasetDict({\n",
    "            \"train\":Dataset.from_pandas(training_set,features=ner_features),\n",
    "            \"test\":Dataset.from_pandas(test_set,features=ner_features),\n",
    "            \"validation\":Dataset.from_pandas(valid_set,features=ner_features)\n",
    "        })\n",
    "    \n",
    "    return ner_dataset, class_names_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3f232aa6-c5de-4f63-b092-13aa5bc5f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = pd.read_csv(\"data/NER_TRAINING_SET_2021_08_30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a68fcfab-ad2a-4312-981d-051b8b93307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>for</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>country</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>B-REG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>for</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>second</td>\n",
       "      <td>B-DATE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    words  labels\n",
       "0            0      for       0\n",
       "1            0  country       0\n",
       "2            0  Ireland   B-REG\n",
       "3            0      for       0\n",
       "4            0   second  B-DATE"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84a9aaf6-4e0f-441c-931a-68bbadcb3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset, class_names_map = prepare_ner_training_set(ner_data,test_fraction=0.1,valid_fraction=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fd5da928-a9c8-4ab0-ba83-3b78f7d18ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a76a5a46-84c2-4829-87fc-2975650ad277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-REG': 1,\n",
       " 'B-DATE': 2,\n",
       " 'I-DATE': 3,\n",
       " 'B-PLAT': 4,\n",
       " 'I-PLAT': 5,\n",
       " 'B-CAT': 6,\n",
       " 'I-CAT': 7,\n",
       " 'B-MET': 8,\n",
       " 'I-MET': 9,\n",
       " 'B-APP': 10,\n",
       " 'I-APP': 11,\n",
       " 'I-REG': 12}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a79665-91f3-4cf4-a953-cf37772b4ac3",
   "metadata": {},
   "source": [
    "#### Show some random elements of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "27fe8afa-75fd-4c0f-84da-c227d0c33029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "14065157-fda3-4aee-be0f-b5fde55bcd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2920</td>\n",
       "      <td>[what, are, for, 주선, for, kakao, the, best, featuring, vs, dau, apple, ios, store, compared, to, apple, for, first, quarter, 2016]</td>\n",
       "      <td>[O, O, O, B-APP, I-APP, I-APP, O, O, B-MET, O, B-MET, B-PLAT, I-PLAT, I-PLAT, O, O, B-PLAT, O, B-DATE, I-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1390</td>\n",
       "      <td>[for, casino, the, best, installs, for, country, el, salvador, on, Apple, Play, compared, to, play, store, q2, 2013]</td>\n",
       "      <td>[O, B-CAT, O, O, B-MET, O, O, B-REG, I-REG, O, B-PLAT, I-PLAT, O, O, B-PLAT, I-PLAT, B-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550</td>\n",
       "      <td>[tell, me, about, for, category, action, platformer, game, 2009, last, quarter, for, Apple, Store, for, country, Pakistan, the, best, base, of, users, and, 7, day, retention, top, chart, ranks, in, canada, in, october, 2019]</td>\n",
       "      <td>[O, O, O, O, O, B-CAT, I-CAT, I-CAT, B-DATE, I-DATE, I-DATE, O, B-PLAT, I-PLAT, O, O, B-REG, O, O, B-MET, I-MET, I-MET, O, B-MET, I-MET, I-MET, B-MET, I-MET, I-MET, O, B-REG, O, B-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1650</td>\n",
       "      <td>[tell, me, about, in, El, Salvador, for, app, 银河足球队, and, 贵族：1896, hottest, by, dau, and, install, base]</td>\n",
       "      <td>[O, O, O, O, B-REG, I-REG, O, O, B-APP, O, B-APP, O, O, B-MET, O, B-MET, I-MET]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3576</td>\n",
       "      <td>[what, are, for, fish, hunter, games, applications, for, avg, time, per, user, for, country, Latvia, on, apple, store, and, google, play, in, since, 2010]</td>\n",
       "      <td>[O, O, O, B-CAT, I-CAT, I-CAT, O, O, B-MET, I-MET, I-MET, I-MET, O, O, B-REG, O, B-PLAT, I-PLAT, O, B-PLAT, I-PLAT, O, B-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>123</td>\n",
       "      <td>[what, are, in, hidden, object, game, apps, best, by, highest, usage, vs, most, users, for, apple, ios, store, and, apple, |what, are, zoom's, ratings, on, android, this, year?]</td>\n",
       "      <td>[O, O, O, B-CAT, I-CAT, I-CAT, O, O, O, B-MET, I-MET, O, B-MET, I-MET, O, B-PLAT, I-PLAT, I-PLAT, O, B-PLAT, B-APP, O, B-APP, B-MET, O, B-PLAT, B-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4539</td>\n",
       "      <td>[for, android, in, 2011, since, highest, top, mobile, apps, safari专用adblock, plus, vs, kardia, turks, and, caicos]</td>\n",
       "      <td>[O, B-PLAT, O, B-DATE, I-DATE, O, B-MET, I-MET, I-MET, B-APP, I-APP, O, B-APP, B-REG, I-REG, I-REG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>525</td>\n",
       "      <td>[what, are, strategy, game, apps, the, best, paid, search, Grenada, for, Apple, Play, compared, to, android, store, in, q1, 2009, starbucks, ad, revenue, q1, 2019]</td>\n",
       "      <td>[O, O, B-CAT, I-CAT, O, O, O, B-MET, I-MET, B-REG, O, B-PLAT, I-PLAT, O, O, B-PLAT, I-PLAT, O, B-DATE, I-DATE, B-APP, B-MET, I-MET, B-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2736</td>\n",
       "      <td>[what, are, livraison, repas, and, i̇nternetsiz, okey, the, best, highest, wau, for, Ios, for, last, quarter]</td>\n",
       "      <td>[O, O, B-APP, I-APP, O, B-APP, I-APP, O, O, B-MET, I-MET, O, B-PLAT, O, B-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2139</td>\n",
       "      <td>[what, are, for, ゼロから始める異世界生活, リゼロス, Lost, In, Memories, hottest, by, top, grossing, have, in, italy, for, Android, Store, vs, apple, ios, store, q2, 2015]</td>\n",
       "      <td>[O, O, O, B-APP, I-APP, I-APP, I-APP, I-APP, O, O, B-MET, I-MET, O, O, B-REG, O, B-PLAT, I-PLAT, O, B-PLAT, I-PLAT, I-PLAT, B-DATE, I-DATE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(ner_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "144a535a-8118-4d20-9381-8d68b759416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    \n",
    "    label_all_tokens = True\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b25c5f-5eea-4d4d-a65d-6aa9f432994e",
   "metadata": {},
   "source": [
    "#### Some NER models that we could experiment with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65636e67-e352-4c2e-8ff0-03d4fc900362",
   "metadata": {},
   "source": [
    "#### How to save and reload a model\n",
    "https://huggingface.co/transformers/main_classes/model.html\n",
    "\n",
    "This one has already been trained on conll03, maybe it would be a good starting point?\n",
    "https://huggingface.co/elastic/distilbert-base-cased-finetuned-conll03-english\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "23e4ccfa-b31b-43d1-92b1-9430a3b4738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7ee13755-fd0c-411c-ae6d-b88857f8a891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/rmartinshort/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /Users/rmartinshort/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /Users/rmartinshort/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/rmartinshort/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/rmartinshort/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7122d236-8bc1-42b7-aefe-4e99efd89017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0e14ab0f26458dbe9fc40d49f41bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427af73dd3ea43008f598bc8a31a7365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5772240503dc46099b38cd71aa689323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ner_dataset = ner_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a4c747ec-1ecc-40ed-b91d-37a410dd4586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'tokens'],\n",
       "        num_rows: 5130\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'tokens'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'tokens'],\n",
       "        num_rows: 270\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ner_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "53837b0e-d8f5-4029-858e-1b8acd0a76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that we don't need in the training part\n",
    "tokenized_ner_dataset = tokenized_ner_dataset.remove_columns(\n",
    "    [\"ner_tags\", \"tokens\",\"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5f872dc0-a048-441b-9806-1725d35affae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(class_names_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8cbba13f-93a7-4d45-b19d-0b0fad4e6c12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/rmartinshort/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/rmartinshort/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "629ef7af-d831-4335-b80f-25a477c3a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.label2id = class_names_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5dbaca76-d4f3-4da3-97b9-87763d8a7317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-cased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"B-APP\": 10,\n",
       "    \"B-CAT\": 6,\n",
       "    \"B-DATE\": 2,\n",
       "    \"B-MET\": 8,\n",
       "    \"B-PLAT\": 4,\n",
       "    \"B-REG\": 1,\n",
       "    \"I-APP\": 11,\n",
       "    \"I-CAT\": 7,\n",
       "    \"I-DATE\": 3,\n",
       "    \"I-MET\": 9,\n",
       "    \"I-PLAT\": 5,\n",
       "    \"I-REG\": 12,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "151ef742-aa31-4a35-8461-d09b288fff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "46ec5c47-34c8-44bf-831f-04f132c9f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-v1-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08ccd354-f5d4-48bf-a6db-a6a8c404b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = datasets.load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "16d02a48-da1a-40a5-a7f6-214442e390f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [class_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [class_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63deb14f-27d8-44c4-bde0-ef1800b7ce28",
   "metadata": {},
   "source": [
    "### Test that the model will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "62283151-8f2f-4579-b7e0-bdaebe2e14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_ner_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_ner_dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2c0335e8-95e7-446a-ab7b-3d46bc55d6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8, 49]),\n",
       " 'input_ids': torch.Size([8, 49]),\n",
       " 'attention_mask': torch.Size([8, 49])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "14b257df-55f2-4d6b-a199-310a8fafa052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6243, grad_fn=<NllLossBackward0>) torch.Size([8, 49, 13])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f621c1-be4b-4178-afa5-74b21211ec54",
   "metadata": {},
   "source": [
    "### Set up trainer and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9d493c7d-40d6-4efb-95e0-aea1cb989a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_ner_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_ner_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ca1a5129-26e1-456b-9279-180fe1c26d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5130\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 805\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='805' max='805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [805/805 1:30:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061897</td>\n",
       "      <td>0.961669</td>\n",
       "      <td>0.946180</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.978567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053096</td>\n",
       "      <td>0.969255</td>\n",
       "      <td>0.953644</td>\n",
       "      <td>0.961386</td>\n",
       "      <td>0.981588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.046277</td>\n",
       "      <td>0.969315</td>\n",
       "      <td>0.967983</td>\n",
       "      <td>0.968649</td>\n",
       "      <td>0.985328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.047760</td>\n",
       "      <td>0.972978</td>\n",
       "      <td>0.968965</td>\n",
       "      <td>0.970967</td>\n",
       "      <td>0.986263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.046551</td>\n",
       "      <td>0.967856</td>\n",
       "      <td>0.969947</td>\n",
       "      <td>0.968900</td>\n",
       "      <td>0.985256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to distilbert-base-cased-finetuned-v1-ner/checkpoint-500\n",
      "Configuration saved in distilbert-base-cased-finetuned-v1-ner/checkpoint-500/config.json\n",
      "Model weights saved in distilbert-base-cased-finetuned-v1-ner/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-cased-finetuned-v1-ner/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-cased-finetuned-v1-ner/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=805, training_loss=0.03611059899655929, metrics={'train_runtime': 5427.8438, 'train_samples_per_second': 4.726, 'train_steps_per_second': 0.148, 'total_flos': 316602755800320.0, 'train_loss': 0.03611059899655929, 'epoch': 5.0})"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e19a9d07-183c-4932-86ef-77457fbdddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.04655105248093605,\n",
       " 'eval_precision': 0.9678557428459428,\n",
       " 'eval_recall': 0.9699469652327637,\n",
       " 'eval_f1': 0.9689002256450506,\n",
       " 'eval_accuracy': 0.9852560414269275,\n",
       " 'eval_runtime': 38.0201,\n",
       " 'eval_samples_per_second': 15.781,\n",
       " 'eval_steps_per_second': 0.5,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "92423757-6386-4070-b939-121095f67098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 270\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'APP': {'precision': 0.877431906614786,\n",
       "  'recall': 0.898406374501992,\n",
       "  'f1': 0.8877952755905512,\n",
       "  'number': 502},\n",
       " 'CAT': {'precision': 0.976,\n",
       "  'recall': 0.991869918699187,\n",
       "  'f1': 0.9838709677419355,\n",
       "  'number': 123},\n",
       " 'DATE': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 209},\n",
       " 'MET': {'precision': 0.9884105960264901,\n",
       "  'recall': 0.9835255354200988,\n",
       "  'f1': 0.9859620148637489,\n",
       "  'number': 607},\n",
       " 'PLAT': {'precision': 0.9969879518072289,\n",
       "  'recall': 0.993993993993994,\n",
       "  'f1': 0.9954887218045113,\n",
       "  'number': 333},\n",
       " 'REG': {'precision': 0.9770114942528736,\n",
       "  'recall': 0.9883720930232558,\n",
       "  'f1': 0.9826589595375722,\n",
       "  'number': 344},\n",
       " 'overall_precision': 0.9615384615384616,\n",
       " 'overall_recall': 0.9678942398489141,\n",
       " 'overall_f1': 0.9647058823529413,\n",
       " 'overall_accuracy': 0.9846955936764211}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_ner_dataset[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bfba6dba-0622-453c-8b20-88f740c9a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in distilbert-base-case-finetuned-app-ner-tok/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-case-finetuned-app-ner-tok/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('distilbert-base-case-finetuned-app-ner-tok/tokenizer_config.json',\n",
       " 'distilbert-base-case-finetuned-app-ner-tok/special_tokens_map.json',\n",
       " 'distilbert-base-case-finetuned-app-ner-tok/vocab.txt',\n",
       " 'distilbert-base-case-finetuned-app-ner-tok/added_tokens.json',\n",
       " 'distilbert-base-case-finetuned-app-ner-tok/tokenizer.json')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"distilbert-base-case-finetuned-app-ner-tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "84888bb6-eaad-4b72-9c2f-6d72a3a6d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in distilbert-base-case-finetuned-app-ner/config.json\n",
      "Model weights saved in distilbert-base-case-finetuned-app-ner/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"distilbert-base-case-finetuned-app-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f96f73fd-bd69-42d6-8b8f-316daa2b2541",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file distilbert-base-case-finetuned-app-ner/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-APP\": 10,\n",
      "    \"B-CAT\": 6,\n",
      "    \"B-DATE\": 2,\n",
      "    \"B-MET\": 8,\n",
      "    \"B-PLAT\": 4,\n",
      "    \"B-REG\": 1,\n",
      "    \"I-APP\": 11,\n",
      "    \"I-CAT\": 7,\n",
      "    \"I-DATE\": 3,\n",
      "    \"I-MET\": 9,\n",
      "    \"I-PLAT\": 5,\n",
      "    \"I-REG\": 12,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file distilbert-base-case-finetuned-app-ner/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at distilbert-base-case-finetuned-app-ner.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-case-finetuned-app-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cea655b2-6fba-46fd-b8ec-7d62c5bed64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_dict = loaded_model.config.id2label\n",
    "label2id_dict = loaded_model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b767f7f5-3020-4982-83ea-205a3da45bf9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file distilbert-base-case-finetuned-app-ner-tok/added_tokens.json. We won't load it.\n",
      "loading file distilbert-base-case-finetuned-app-ner-tok/vocab.txt\n",
      "loading file distilbert-base-case-finetuned-app-ner-tok/tokenizer.json\n",
      "loading file None\n",
      "loading file distilbert-base-case-finetuned-app-ner-tok/special_tokens_map.json\n",
      "loading file distilbert-base-case-finetuned-app-ner-tok/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-case-finetuned-app-ner-tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "65f67f4f-07be-4868-9ff4-214583434e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(\"ner\", grouped_entities=True, model=loaded_model,tokenizer=loaded_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2f922bbc-103d-4e4c-a961-50fbedc8d306",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    }
   ],
   "source": [
    "op = ner_pipe([\"tell me about snapchat downloads in 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c1e31eef-284e-4ae0-969a-1308461fa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label_2_id = {v:k for k,v in id2label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a9db1cd3-3e4f-4c24-937f-f55ad77c4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_2_label = {v:k for k,v in label2id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f9f99d8e-35f9-4658-800c-067ca22c5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(model_output):\n",
    "\n",
    "    ents_data = {\"APP\":[],\"REG\":[],\"DATE\":[],\"PLAT\":[],\"MET\":[],\"CAT\":[]}\n",
    "    for ent in model_output[0]:\n",
    "        ent_id = model_label_2_id[ent['entity_group']]\n",
    "        ent_label = model_id_2_label[ent_id]\n",
    "    \n",
    "        if \"-\" in ent_label:\n",
    "            ent_label = ent_label.split(\"-\")[1]\n",
    "            ents_data[ent_label].append(ent[\"word\"])\n",
    "    \n",
    "    return ents_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6c8516b7-e234-4ace-8f70-457a13606463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'APP': ['clash', 'of clans'],\n",
       " 'REG': ['japan'],\n",
       " 'DATE': ['last', 'year'],\n",
       " 'PLAT': ['google'],\n",
       " 'MET': ['mobile users'],\n",
       " 'CAT': []}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"clash of clans mobile users in japan on google last year\"\n",
    "op = ner_pipe([query])\n",
    "extract_entities(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c4a97-3893-45f3-9bc9-dd937a9b9ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
