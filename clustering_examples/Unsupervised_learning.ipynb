{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Clustering with traditional ML\n",
    "\n",
    "## Clusters as classes \n",
    "\n",
    "- Divide the dataset into groups \n",
    "- Clustering for its own sake - looking for structure\n",
    "\n",
    "## Clusters as projections \n",
    "\n",
    "- This helps to reduce dimensionality or compress datasets to make it easier to run other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans \n",
    "\n",
    "- General purpose\n",
    "- Scalable to large N\n",
    "- Even cluster size \n",
    "\n",
    "However \n",
    "\n",
    "- Need to specify K\n",
    "- Distance metric assumes hyperspherical clusters \n",
    "- Assumes the features are not correlated \n",
    "- Sensitive to outliers\n",
    "\n",
    "### Potential ouputs of clustering\n",
    "\n",
    "- Hierachy of clusters - for a taxonomy\n",
    "- Exclusive vs multiple memberships\n",
    "- Hard vs. soft assigmnent to clusters (e.g. for soft clustering we might assign probabilities) \n",
    "- Complete vs. incomplete \n",
    "\n",
    "### What should a cluster look like?\n",
    "\n",
    "- Clearly separate groupings \n",
    "- Center based (compact)\n",
    "- Density based (rather than each datapoint being within some distance of a center, represented by proximity to similar)\n",
    "- Tree based\n",
    "\n",
    "## Gaussian Mixture models\n",
    "\n",
    "- Estimate the mean and std of the normal distributions that are assumed to have made the dataset. This is a soft assignment\n",
    "\n",
    "## Hierachical clustering\n",
    "\n",
    "- A bottom-up approach that asserts that data can be sucessively merged into increasingly dissimilar clusters\n",
    "- This does not require a specified K because it involves an incremental process\n",
    "\n",
    "## Density based clustering \n",
    "\n",
    "- Cluster membership is based on proximity to other cluster members\n",
    "- The goal is to identify dense regions, allowing the algorithm to find various shapes \n",
    "- This does not work well in high dimensions\n",
    "\n",
    "## Graph/spectal density clustering\n",
    "\n",
    "- Membership is based on a similarity matrix\n",
    "- Finds clusters that are connected but not compact \n",
    "- Great for a small number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering\n",
    "\n",
    "This is a bottom-up approach, merging remaining groups based on similarity. IN practice this is more common than top-down clustering. \n",
    "\n",
    "The algorithm computes a simialrity matrix between all existing items. At N-1 steps it then merges the closest (least dissimilar) items into a new cluster. \n",
    "\n",
    "How to measure similarity:\n",
    "\n",
    "- Single link (nearest neighbor distance). Can make elongated clusters\n",
    "- Complete link (max distance). Produced more spherical clusters\n",
    "- Group average \n",
    "- Ward method, which is centroid-based \n",
    "\n",
    "### Dendrogram\n",
    "\n",
    "- This is a tree-like diagram that descirbes the hierarchcial clustering relationship between datapoints. Each datapoint is a leaf and the final merge is the root. Simialrity monotonically decreases from bottom to top\n",
    "- Clusters can be selected by cutting the dendrogram at some level\n",
    "- Fit can be evaluated with cophenetic correlation: how similar are pairwise distances and similarities at which pairwise merges occur\n",
    "\n",
    "### Pros and cons\n",
    "\n",
    "- Does not persure a global objective, meaning that K does not need to be specified\n",
    "- Produces a potentially useful hierarchy \n",
    "- Intiuitive visulaization\n",
    "- Can be combined with K-means \n",
    "\n",
    "- High computational cost\n",
    "- Merges are final: No global optimum and difficulties with noisy and/or high dimensional data\n",
    "- Dentrogram may be misleading because the algorithm may be sensitive no noise. Sometimes a hierarchical strucutre actually doesn't exist in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density based clustering\n",
    "\n",
    "We'll look at DBSCAN and HDBSCAN. This assumes that clusters consist naturally of high density areas. We have core and non-core samples. The parameteters are radius (eps), which have some specified number of other points within the radius are known as core samples. Min_samples is the minimum number of points that have to be within a radius for a datapoint to be converted into a core sample. \n",
    "\n",
    "Each cluster consists of core samples, points that can be reached. Anything beyond that are known as outliers. The algorithm is deterministic and can find clusters of different shapes, which is helpful. It can also find outliers, which may be useful.\n",
    "\n",
    "However, it may not work well with very high dimensional data and the parameter tuning can be challenging. \n",
    "\n",
    "### HDBSCAN \n",
    "\n",
    "Hierarchical DBSCAN allows density to vary, which may be helpful. It iterates over a range of eps values and then selects the one that provides the most stable clusters. The parameters are min_cluster_size and min_samples. This can provide insight into the density structure\n",
    "\n",
    "## Gaussian mixture models \n",
    "\n",
    "These generalize the kmeans clustering approach by extending it to probability. We assume that the clusters are produced by a collection of guassian distributions. The aim is to estimate the paramters of these distributions that drive the generation of these clusters. It produces a soft assignment for each data point to originate from any of the distributions. \n",
    "\n",
    "It also adds information about the covariance from the centroids. Allows for various covariance assumptions: From full to spherical\n",
    "\n",
    "This is the opposite to the K-means clustering, which can be seen as a hard option. The GM modelling adds in the covariance matrix.\n",
    "\n",
    "Again, the goal is to learn the parameters of the gaussians from the unlabelled data. The means are initialtized as are the covariance matrices. The epectation step then provides probabilities for each point being a part of each distribtution. Then we adjust the parameters of the normal distributions to make it most likely that the assigned points actually come from that distribution. \n",
    "\n",
    "We choose to minimize the Bayesian Information Criterion (BIC), the first term is a penalty for parameter number and the second is a maxiumum likelihood function. The k here is the number of clusters so we see that is penalizes a large number of clusters\n",
    "\n",
    "$$ \\text{ln}(n)k - 2\\text{ln}(L) $$\n",
    "\n",
    "This assumes that the data are distributed iid and is only valid for large N. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Topic modelling \n",
    "\n",
    "Topic modelling is typically used for NLP tasks, which finds groups of words that are most commonly associated with particular topics. \n",
    "\n",
    "The goal is to capture both the semantics and the words themselves. The main goal is usually to annotate documents in order to manage and summarize them and to recommend content. The first advances in this field were related to bag of words implementations\n",
    "\n",
    "### Term-Document matrix\n",
    "\n",
    "This is BOW model that transforms text into a matrix. Each row corresponds to a document and each column corresponds to a word. Thus the documents are represented as a sequence of numbers and its possible to determine similarity etc. \n",
    "\n",
    "The issue is that therre is a very large number of unique terms so these matrices become very sparse. One approach to make this more manageable is to use stemming or lemmitization, which reduces words to their common roots. Once we have words represented as vectors we can do things like determine the cosine distance between words, documents or queries. \n",
    "\n",
    "BOW techniques are a great first start in NLP problems, but they tend to ingnore word order and thus result in some loss of content and semantics. BOW cannot capture the fact that some word can have different meanings, or that multiple words can have the same meaning. \n",
    "\n",
    "### Latent Semantic Indexing\n",
    "\n",
    "This is an extension where we try to discover previously unknown topics from the term-document matrix. We decompose the term-document matrix using SVD to reduct the dimensionalty and gather the most important terms in the singular value decomposition component. Note $ A = U \\Sigma V^T $. We can do this to determine the best k lower rank approximation to the matrix A. This is basically what PCA is doing, because we're keeping k of the singular values and setting all the others to zero. \n",
    "\n",
    "- U matrix: Document-topic similarity\n",
    "- Singular values: Concept strength \n",
    "- V^T matrix: Term-concept similarity \n",
    "\n",
    "#### Pros of LSI:\n",
    "- Can be used for dimensionality reduction\n",
    "- Captures some semantics of text data, allowing content to be clustered\n",
    "\n",
    "#### Cons of LSI:\n",
    "- Hard to interpret because some singular values can be negative\n",
    "- No probabilistic model and thus hard to evaluate fit\n",
    "\n",
    "### Probabalistic LSI and moving towards LDA\n",
    "\n",
    "This makes some assumptions about the process that has generated the topics. Thus it is a generative model. The thought is that the words that we see are samples from the topics and each docuemnt consists of various proportions of different topics. Thus, each topic has a probability distribution over words. \n",
    "\n",
    "How do we estimate these probability distributions? Use expectation maximization to estimate their parameters. \n",
    "\n",
    "LDA extends this idea to the document layer itself. As we generate the documents, we are sampling them from a Dirchlet distribution. This makes a three-level model: number of words, mix of topics and word choice. For each document we have n words and we estimate the parameters of the Dirichlet distribution that gave rise to the assignment of words to each document. We also have multinomial distirbution that assigns the words the their topics. This allows us to assign topic probabilities to documents, and each topic is assiocated with a probability distribution over words (some words are more likley to be drawn than others) \n",
    "\n",
    "There is a parameter $\\alpha$, which control the extent to which one topic is favored over the others. The smaller this value, the more dominent any one of the topics becomes. \n",
    "\n",
    "#### Pros\n",
    "- Produces topics that humans can relate to\n",
    "- Fully generative\n",
    "- Probabalistic and can be extensible to include other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold and deep learning \n",
    "\n",
    "This will is designed for high dimensional data. \n",
    "\n",
    "High dimensional data is challenging\n",
    "\n",
    "- Curse of dimensionalty because the space becomes sparse. No real local neighborhood\n",
    "- All data could be near the boundary of the sample space\n",
    "- No local neighborhood: 1% of unit hypercube volume in 10D is 63% of the input range. If we want even sampling, we need to use a huge number of datapoints, typically increasing exponentially with the dimension of the space, n. \n",
    "- The complexity of estimators increases with # dimensions\n",
    "- See https://en.wikipedia.org/wiki/Curse_of_dimensionality for further explanation\n",
    "\n",
    "### What is a manifold?\n",
    "\n",
    "Manifold learning is used mainly for dimensionality reduction, because of the various challenges that exist with this sort of data\n",
    "\n",
    "A manifold is a connected region that looks like Euclidean space. Locally, some spaces can be approximated by lower dimensional spaces. In the manifold hypothesis we assert that data always lie close to a manifold\n",
    "\n",
    "This idea is used in supervised machine learning all the time - the linear separator that is applied in regression, for example, is a manifold because it captures the direction of greatest importance within the dataset. \n",
    "\n",
    "Manifold learning tries to approximate intrinsic dimensions. Since data does not vary randomly and examples connect via transformations, most of the 'action' is really happening on a lower dimensional space. We thus need to find this lower dimensional space and then project the higher dimensional datapoints onto it\n",
    "\n",
    "One good example is that of roads - these are one dimensional ways of exploring 3D \n",
    "\n",
    "### Manifold learning in practice\n",
    "\n",
    "We can apply linear and locally linear manifold learning. The main goal is to capture as much variation in the data as possible. We want to ensure that data that is close in high dimensions remain close in lower dimensions etc. This can be an issue because compression can lead to crowding. \n",
    "\n",
    "### Linear approaches\n",
    "\n",
    "- Princpal component analysis (linear components that maximize variance - SVD of covariance matrix)\n",
    "- Multidimensional scaling (linear embedding that maintains pairwise distances)\n",
    "- These are popular but they do not capture non-linear components\n",
    "\n",
    "### Nonlinear approaches\n",
    "\n",
    "- LLE - locally linear but globally nonlinear\n",
    "-> Find K nearest neighbors to each point\n",
    "-> Represent each data point as optimal linear combinations of its neighbors\n",
    "-> Map linear combinations to lower-dimensional space\n",
    "\n",
    "\n",
    "This is really good at unrolling manifolds but has a harder time dealing with data that is not uniformly distributed. If K is larger than the number of dimensions then there may be some issues. \n",
    "\n",
    "### t-SNE and UMAP algorithms\n",
    "\n",
    "- t-SNE is t-distibuted Stochastic Neighbor Embedding\n",
    "- UMAP is Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "These are non-linear approaches to high-dimensional visualzation.\n",
    "\n",
    "t-SNE assumes that data are on several different, but low-dimesional manifolds. Its aim is to keep similar points together, which is a weakness of LLE. Its main challenge is to be capable of representing both local and global strucutre in the points. Its basically developed to try to deal with crowding problems in lower dimensional space. Local structure is difficult to tease out\n",
    "\n",
    "The key parameter is the perplexity - how well a probablity distributions predicts a given sample - the number of neighbors that is used to infer the location of a datapoint in this high dimensional space\n",
    "\n",
    "\n",
    "#### UMAP\n",
    "\n",
    "- Much faster than t-SNE and used for general dimensionality reduction in addition to visualization\n",
    "- Can use cosine similariy and various distance functions\n",
    "- Assumes that the data is locally uniformly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
